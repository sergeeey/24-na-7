"""
–§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –≤—ã—Ö–æ–¥–∞ –Ω–∞ 80% –ø–æ–∫—Ä—ã—Ç–∏—è.
–¶–µ–ª–µ–≤—ã–µ –≤–µ—Ç–∫–∏: api/main, storage/migrate, metrics/prometheus.
"""
import sqlite3
from unittest.mock import patch, MagicMock

from fastapi.testclient import TestClient


def test_migrate_main_verify():
    """storage.migrate.main() —Å --verify –≤—ã–∑—ã–≤–∞–µ—Ç verify_row_counts –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0."""
    from unittest.mock import patch

    with patch("sys.argv", ["migrate", "--verify"]):
        with patch("src.storage.migrate.verify_row_counts", return_value={"match": True, "tables": {}}):
            from src.storage.migrate import main
            exit_code = main()
    assert exit_code == 0


def test_migrate_main_no_args_returns_1():
    """storage.migrate.main() –±–µ–∑ —Ñ–ª–∞–≥–æ–≤ –≤—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Å–∫–∞–∑–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 1."""
    with patch("sys.argv", ["migrate"]):
        from src.storage.migrate import main
        exit_code = main()
    assert exit_code == 1


def test_migrate_main_apply_schema():
    """storage.migrate.main() —Å --apply-schema –≤—ã–∑—ã–≤–∞–µ—Ç apply_schema_migrations."""
    with patch("sys.argv", ["migrate", "--apply-schema", "--to", "sqlite"]):
        with patch("src.storage.migrate.apply_schema_migrations", return_value={"migrations_applied": [], "errors": []}):
            from src.storage.migrate import main
            exit_code = main()
    assert exit_code == 0


def test_migrate_main_migrate_data_to_sqlite_returns_1():
    """storage.migrate.main() —Å --migrate-data --to sqlite –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 1 (only Supabase)."""
    with patch("sys.argv", ["migrate", "--migrate-data", "--to", "sqlite"]):
        from src.storage.migrate import main
        exit_code = main()
    assert exit_code == 1


def test_migrate_main_migrate_data_failed_returns_1():
    """storage.migrate.main() —Å --migrate-data –ø—Ä–∏ status=failed –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 1."""
    with patch("sys.argv", ["migrate", "--migrate-data", "--to", "supabase"]):
        with patch("src.storage.migrate.migrate_to_supabase", return_value={"status": "failed", "errors": ["x"]}):
            from src.storage.migrate import main
            exit_code = main()
    assert exit_code == 1


def test_migrate_main_verify_with_differences():
    """storage.migrate.main() —Å --verify –ø—Ä–∏ match=False –≤—ã–≤–æ–¥–∏—Ç —Ä–∞–∑–ª–∏—á–∏—è."""
    with patch("sys.argv", ["migrate", "--verify"]):
        with patch("src.storage.migrate.verify_row_counts", return_value={
            "match": False, "tables": {}, "differences": [{"table": "t1", "sqlite": 1, "supabase": 0, "diff": 1}]
        }):
            from src.storage.migrate import main
            exit_code = main()
    assert exit_code == 0


def test_migrate_main_apply_schema_with_errors():
    """storage.migrate.main() —Å --apply-schema –ø—Ä–∏ errors –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≤—ã–≤–æ–¥–∏—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ."""
    with patch("sys.argv", ["migrate", "--apply-schema", "--to", "sqlite"]):
        with patch("src.storage.migrate.apply_schema_migrations", return_value={
            "migrations_applied": ["001.sql"], "errors": ["001.sql: some error"]
        }):
            from src.storage.migrate import main
            exit_code = main()
    assert exit_code == 0


def test_migrate_main_backup_fails():
    """storage.migrate.main() —Å --backup –∏ --migrate-data –ø—Ä–∏ –æ—à–∏–±–∫–µ backup –Ω–µ –ø–∞–¥–∞–µ—Ç."""
    with patch("sys.argv", ["migrate", "--backup", "--migrate-data", "--to", "supabase"]):
        with patch("src.storage.migrate.backup_sqlite", side_effect=FileNotFoundError("no db")):
            with patch("src.storage.migrate.migrate_to_supabase", return_value={"status": "ok"}):
                from src.storage.migrate import main
                exit_code = main()
    assert exit_code == 0


def test_api_ingest_status_endpoint():
    """GET /ingest/status/{file_id} –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç pending."""
    from src.api.main import app

    client = TestClient(app)
    r = client.get("/ingest/status/some-file-id-123")
    assert r.status_code == 200
    data = r.json()
    assert data.get("status") == "pending"
    assert data.get("id") == "some-file-id-123"


def test_api_prometheus_metrics_endpoint():
    """GET /metrics/prometheus –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 200 –∏ —Ç–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Prometheus."""
    from src.api.main import app

    client = TestClient(app)
    r = client.get("/metrics/prometheus")
    assert r.status_code == 200
    assert "reflexio_uploads_total" in r.text or "reflexio_health" in r.text


def test_api_prometheus_metrics_db_exception(tmp_path):
    """GET /metrics/prometheus –ø—Ä–∏ –æ—à–∏–±–∫–µ SQLite –Ω–µ –ø–∞–¥–∞–µ—Ç (except pass)."""
    from src.api.main import app
    from src.utils.config import settings

    (tmp_path / "reflexio.db").write_bytes(b"")
    with patch.object(settings, "STORAGE_PATH", tmp_path):
        with patch("sqlite3.connect") as mock_connect:
            mock_conn = MagicMock()
            mock_conn.cursor.return_value.execute.side_effect = sqlite3.Error("db error")
            mock_connect.return_value = mock_conn
            client = TestClient(app)
            r = client.get("/metrics/prometheus")
    assert r.status_code == 200


def test_api_prometheus_metrics_with_osint_json(tmp_path, monkeypatch):
    """GET /metrics/prometheus —Å cursor-metrics.json —Å osint –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤–µ—Ç–∫—É deepconf."""
    from src.api.main import app

    (tmp_path / "cursor-metrics.json").write_text(
        '{"metrics": {"osint": {"avg_deepconf_confidence": 0.85}}}',
        encoding="utf-8",
    )
    monkeypatch.chdir(tmp_path)
    client = TestClient(app)
    r = client.get("/metrics/prometheus")
    assert r.status_code == 200
    assert "reflexio_deepconf_avg_confidence" in r.text


def test_apply_schema_sqlite_execute_exception(tmp_path):
    """apply_schema_migrations(backend=sqlite) –ø—Ä–∏ –æ—à–∏–±–∫–µ executescript –ø–∏—à–µ—Ç –≤ errors."""
    from src.storage.migrate import apply_schema_migrations
    from src.utils.config import settings

    (tmp_path / "reflexio.db").write_bytes(b"")
    with patch.object(settings, "STORAGE_PATH", tmp_path):
        with patch("sqlite3.connect") as mock_connect:
            mock_conn = MagicMock()
            mock_cursor = MagicMock()
            mock_cursor.executescript.side_effect = sqlite3.OperationalError("syntax error")
            mock_conn.cursor.return_value = mock_cursor
            mock_connect.return_value = mock_conn
            result = apply_schema_migrations(backend="sqlite")
    assert "errors" in result
    assert any("syntax error" in e or "Failed" in e for e in result["errors"])


def test_api_voice_intent_endpoint():
    """POST /voice/intent –±–µ–∑ —Ç–µ–ª–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 422 –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –æ—à–∏–±–∫—É."""
    from src.api.main import app

    client = TestClient(app)
    r = client.post("/voice/intent", json={})
    assert r.status_code in (200, 400, 422, 500)


def test_api_search_phrases_exception_returns_500():
    """POST /search/phrases –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏ –≤ search_phrases –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 500."""
    from src.api.main import app

    with patch("src.api.routers.search.search_phrases", side_effect=RuntimeError("search failed")):
        client = TestClient(app)
        r = client.post("/search/phrases", json={"query": "test", "audio_id": "a1"})
    assert r.status_code == 500
    assert "Search failed" in r.json().get("detail", "")


def test_api_voice_intent_exception_returns_500():
    """POST /voice/intent –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏ –≤ recognize_intent –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 500."""
    from src.api.main import app

    with patch("src.voice_agent.voiceflow_rag.get_voiceflow_client") as m:
        m.return_value.recognize_intent.side_effect = RuntimeError("voiceflow down")
        client = TestClient(app)
        r = client.post("/voice/intent", json={"text": "hello"})
    assert r.status_code == 500
    assert "Intent recognition failed" in r.json().get("detail", "")


def test_metrics_osint_non_dict_exception(tmp_path, monkeypatch):
    """GET /metrics –ø—Ä–∏ cursor-metrics.json —Å osint –Ω–µ-—Å–ª–æ–≤–∞—Ä—ë–º –Ω–µ –ø–∞–¥–∞–µ—Ç (except 656-657)."""
    from src.api.main import app

    (tmp_path / "cursor-metrics.json").write_text(
        '{"metrics": {"osint": 123}}',
        encoding="utf-8",
    )
    monkeypatch.chdir(tmp_path)
    client = TestClient(app)
    r = client.get("/metrics")
    assert r.status_code == 200


def test_digest_generate_extended_metrics_invalid_created_at(tmp_path):
    """DigestGenerator.generate —Å EXTENDED_METRICS –∏ invalid created_at –Ω–µ –ø–∞–¥–∞–µ—Ç (except pass)."""
    from src.digest.generator import DigestGenerator
    from datetime import date
    from src.utils.config import settings

    db_path = tmp_path / "e.db"
    db_path.write_bytes(b"")
    gen = DigestGenerator(db_path=db_path)
    metrics = {
        "transcriptions_count": 1, "facts_count": 0, "total_duration_minutes": 0,
        "total_characters": 1, "total_words": 1, "average_words_per_transcription": 1,
        "information_density_score": 0.0, "density_level": "üü¢ –ù–∏–∑–∫–∞—è",
    }
    with patch.object(settings, "EXTENDED_METRICS", True):
        with patch.object(gen, "get_transcriptions", return_value=[{"text": "x", "created_at": "invalid"}]):
            with patch.object(gen, "extract_facts", return_value=[]):
                with patch.object(gen, "calculate_metrics", return_value=metrics):
                    out = gen.generate(target_date=date(2026, 1, 1), output_format="markdown")
    assert out.exists()


def test_digest_generate_markdown_enhanced_summary_exception(tmp_path):
    """DigestGenerator: –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏ –≤ enhanced summary –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è warning, –º–∞—Ä–∫–¥–∞—É–Ω –Ω–µ –ø–∞–¥–∞–µ—Ç."""
    from src.digest.generator import DigestGenerator
    from datetime import date

    db_path = tmp_path / "d.db"
    db_path.write_bytes(b"")
    gen = DigestGenerator(db_path=db_path)
    metrics = {
        "transcriptions_count": 1, "facts_count": 0, "total_duration_minutes": 0,
        "total_characters": 10, "total_words": 2, "average_words_per_transcription": 2,
        "information_density_score": 0.0, "density_level": "üü¢ –ù–∏–∑–∫–∞—è",
    }
    with patch("src.digest.generator.SUMMARIZER_AVAILABLE", True):
        with patch("src.digest.generator.generate_dense_summary", side_effect=RuntimeError("dense failed")):
            with patch.object(gen, "get_transcriptions", return_value=[{"text": "Hello world", "id": "1"}]):
                with patch.object(gen, "extract_facts", return_value=[]):
                    with patch.object(gen, "calculate_metrics", return_value=metrics):
                        out = gen.generate(target_date=date(2026, 1, 1), output_format="markdown")
    assert out.exists()
