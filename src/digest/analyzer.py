"""–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤."""
from datetime import date
from typing import Dict
import sqlite3
from pathlib import Path

from src.utils.config import settings
from src.utils.logging import setup_logging, get_logger
from src.digest.metrics_ext import calculate_extended_metrics, interpret_semantic_density, interpret_wpm_rate

setup_logging()
logger = get_logger("digest.analyzer")


class InformationDensityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–Ω—è."""
    
    def __init__(self, db_path: Path | None = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞."""
        if db_path is None:
            db_path = settings.STORAGE_PATH / "reflexio.db"
        self.db_path = db_path
    
    def analyze_day(self, target_date: date) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–Ω—è.
        
        Args:
            target_date: –î–∞—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        if not self.db_path.exists():
            logger.warning("database_not_found", db_path=str(self.db_path))
            return self._empty_analysis(target_date)
        
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row
        
        try:
            cursor = conn.cursor()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π
            cursor.execute("""
                SELECT 
                    COUNT(*) as count,
                    SUM(duration) as total_duration,
                    SUM(LENGTH(text)) as total_chars,
                    AVG(LENGTH(text)) as avg_chars
                FROM transcriptions
                WHERE DATE(created_at) = ?
            """, (target_date.isoformat(),))
            
            stats = dict(cursor.fetchone() or {})
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —á–∞—Å–∞–º
            cursor.execute("""
                SELECT 
                    strftime('%H', created_at) as hour,
                    COUNT(*) as count
                FROM transcriptions
                WHERE DATE(created_at) = ?
                GROUP BY hour
                ORDER BY hour
            """, (target_date.isoformat(),))
            
            hourly_distribution = {row[0]: row[1] for row in cursor.fetchall()}
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
            total_duration = stats.get("total_duration") or 0
            total_chars = stats.get("total_chars") or 0
            count = stats.get("count") or 0
            
            density_metrics = self._calculate_density_metrics(
                count, total_duration, total_chars, hourly_distribution
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            transcriptions = []
            if count > 0:
                cursor.execute("""
                    SELECT 
                        id,
                        text,
                        duration,
                        created_at
                    FROM transcriptions
                    WHERE DATE(created_at) = ?
                    ORDER BY created_at ASC
                """, (target_date.isoformat(),))
                transcriptions = [dict(row) for row in cursor.fetchall()]
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            extended_metrics = calculate_extended_metrics(
                transcriptions=transcriptions,
                hourly_distribution=hourly_distribution,
                enabled=getattr(settings, "EXTENDED_METRICS", False),
            )
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            result = {
                "date": target_date.isoformat(),
                "statistics": {
                    "transcriptions_count": count,
                    "total_duration_seconds": total_duration,
                    "total_duration_minutes": round(total_duration / 60, 2) if total_duration else 0,
                    "total_characters": total_chars,
                    "average_characters_per_transcription": round(stats.get("avg_chars") or 0, 1),
                },
                "hourly_distribution": hourly_distribution,
                "density_analysis": density_metrics,
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if extended_metrics:
                result["extended_metrics"] = extended_metrics
                result["extended_metrics"]["interpretation"] = {
                    "semantic_density": interpret_semantic_density(extended_metrics.get("semantic_density", 0)),
                    "wpm_rate": interpret_wpm_rate(extended_metrics.get("wpm_rate", 0)),
                }
            
            logger.info(
                "density_analysis_complete",
                date=target_date.isoformat(),
                density_score=density_metrics.get("score", 0),
            )
            
            return result
            
        finally:
            conn.close()
    
    def _calculate_density_metrics(self, count: int, total_duration: float,
                                   total_chars: int, hourly_distribution: Dict[str, int]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏."""
        
        # –ë–∞–∑–æ–≤–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        time_density = 0.0
        if total_duration > 0:
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –≤ –º–∏–Ω—É—Ç—É
            trans_per_minute = (count / (total_duration / 60)) if total_duration > 0 else 0
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–º–ø ~2 —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏/–º–∏–Ω)
            time_density = min(50, (trans_per_minute / 2) * 50)
        
        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –ø–æ –æ–±—ä—ë–º—É
        volume_density = 0.0
        if count > 0:
            avg_chars = total_chars / count
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É ~200 —Å–∏–º–≤–æ–ª–æ–≤)
            volume_density = min(50, (avg_chars / 200) * 50)
        
        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–º–µ–Ω—å—à–µ –ø–∏–∫–æ–≤ = –≤—ã—à–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å)
        distribution_score = 0.0
        if hourly_distribution:
            hours_with_activity = len(hourly_distribution)
            max_in_hour = max(hourly_distribution.values())
            if max_in_hour > 0:
                # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å = –∫–∞–∫ –º–Ω–æ–≥–æ —á–∞—Å–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–∏–∫–∞
                distribution_score = min(50, (hours_with_activity / max_in_hour) * 25)
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        total_score = time_density * 0.4 + volume_density * 0.4 + distribution_score * 0.2
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å
        level = self._get_density_level(total_score)
        
        return {
            "score": round(total_score, 1),
            "level": level,
            "components": {
                "time_density": round(time_density, 1),
                "volume_density": round(volume_density, 1),
                "distribution_score": round(distribution_score, 1),
            },
            "interpretation": self._interpret_density(total_score, count, total_duration),
        }
    
    def _get_density_level(self, score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏."""
        if score >= 80:
            return "üî¥ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
        elif score >= 60:
            return "üü† –í—ã—Å–æ–∫–∞—è"
        elif score >= 40:
            return "üü° –°—Ä–µ–¥–Ω—è—è"
        elif score >= 20:
            return "üü¢ –ù–∏–∑–∫–∞—è"
        else:
            return "‚ö™ –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"
    
    def _interpret_density(self, score: float, count: int, duration: float) -> str:
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞."""
        if score >= 80:
            return "–û—á–µ–Ω—å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–π –¥–µ–Ω—å —Å –≤—ã—Å–æ–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é"
        elif score >= 60:
            return "–•–æ—Ä–æ—à–∏–π –¥–µ–Ω—å —Å –∞–∫—Ç–∏–≤–Ω—ã–º –æ–±–º–µ–Ω–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"
        elif score >= 40:
            return "–û–±—ã—á–Ω—ã–π –¥–µ–Ω—å —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é"
        elif score >= 20:
            return "–°–ø–æ–∫–æ–π–Ω—ã–π –¥–µ–Ω—å, –º–µ–Ω—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞"
        elif count > 0:
            return "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –≤–æ–∑–º–æ–∂–Ω–æ –ø–∞—É–∑–∞ –∏–ª–∏ —Ñ–æ–∫—É—Å –Ω–∞ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö"
        else:
            return "–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å"
    
    def _empty_analysis(self, target_date: date) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑."""
        return {
            "date": target_date.isoformat(),
            "statistics": {
                "transcriptions_count": 0,
                "total_duration_seconds": 0,
                "total_duration_minutes": 0,
                "total_characters": 0,
                "average_characters_per_transcription": 0,
            },
            "hourly_distribution": {},
            "density_analysis": {
                "score": 0.0,
                "level": "‚ö™ –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
                "components": {
                    "time_density": 0.0,
                    "volume_density": 0.0,
                    "distribution_score": 0.0,
                },
                "interpretation": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å",
            },
        }

