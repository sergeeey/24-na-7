"""Ð“ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚Ð¾Ð² Ð¸Ð· Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð² Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¼ summarization."""
import sqlite3
from datetime import date, datetime
from pathlib import Path
from typing import Optional, Dict, List
import json

from src.utils.config import settings
from src.utils.logging import setup_logging, get_logger
from src.digest.metrics_ext import calculate_extended_metrics
from src.memory.core_memory import get_core_memory
from src.memory.session_memory import get_session_memory

# ÐÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÑƒÐ»Ð¸ summarization (November 2025 Integration Sprint)
try:
    from src.summarizer.chain_of_density import generate_dense_summary
    from src.summarizer.critic import validate_summary
    from src.summarizer.few_shot import extract_tasks, analyze_emotions
    SUMMARIZER_AVAILABLE = True
except ImportError:
    SUMMARIZER_AVAILABLE = False
    logger = get_logger("digest")
    logger.warning("summarizer_modules_not_available", using_basic_summary=True)

setup_logging()
logger = get_logger("digest")


class DigestGenerator:
    """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚ Ð´Ð½Ñ Ð¸Ð· Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²."""
    
    def __init__(self, db_path: Optional[Path] = None):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°."""
        if db_path is None:
            db_path = settings.STORAGE_PATH / "reflexio.db"
        self.db_path = db_path
        self.digests_dir = Path("digests")
        self.digests_dir.mkdir(parents=True, exist_ok=True)
    
    def get_transcriptions(self, target_date: date) -> List[Dict]:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð²ÑÐµ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ Ð·Ð° Ð´ÐµÐ½ÑŒ.
        
        Args:
            target_date: Ð”Ð°Ñ‚Ð° Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸
            
        Returns:
            Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¹ Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
        """
        if not self.db_path.exists():
            logger.warning("database_not_found", db_path=str(self.db_path))
            return []
        
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row
        
        try:
            cursor = conn.cursor()
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ Ð·Ð° Ð´ÐµÐ½ÑŒ
            cursor.execute("""
                SELECT 
                    t.id,
                    t.ingest_id,
                    t.text,
                    t.language,
                    t.language_probability,
                    t.duration,
                    t.segments,
                    t.created_at,
                    i.filename,
                    i.file_size
                FROM transcriptions t
                LEFT JOIN ingest_queue i ON t.ingest_id = i.id
                WHERE DATE(t.created_at) = ?
                ORDER BY t.created_at ASC
            """, (target_date.isoformat(),))
            
            rows = cursor.fetchall()
            transcriptions = [dict(row) for row in rows]
            
            logger.info(
                "transcriptions_found",
                date=target_date.isoformat(),
                count=len(transcriptions),
            )
            
            return transcriptions
            
        finally:
            conn.close()
    
    def extract_facts(self, transcriptions: List[Dict], use_llm: bool = True) -> List[Dict]:
        """
        Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸Ð· Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¹ Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¼ LLM-Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð¼.
        
        Args:
            transcriptions: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¹
            use_llm: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ LLM Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ
        """
        facts = []
        
        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÐµÑÑŒ Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        full_text = " ".join([t.get("text", "").strip() for t in transcriptions if t.get("text", "").strip()])
        
        if not full_text:
            return facts
        
        # Ð•ÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ summarizer, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾
        if use_llm and SUMMARIZER_AVAILABLE:
            try:
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ñ‡ÐµÑ€ÐµÐ· few-shot
                tasks = extract_tasks(full_text)
                for task in tasks:
                    facts.append({
                        "text": task.get("task", ""),
                        "type": "task",
                        "priority": task.get("priority", "medium"),
                        "deadline": task.get("deadline"),
                        "timestamp": transcriptions[0].get("created_at") if transcriptions else None,
                    })
                
                # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¼Ð¾Ñ†Ð¸Ð¸
                emotions = analyze_emotions(full_text)
                if emotions.get("emotions"):
                    facts.append({
                        "text": f"Ð­Ð¼Ð¾Ñ†Ð¸Ð¸: {', '.join(emotions.get('emotions', []))}",
                        "type": "emotion",
                        "intensity": emotions.get("intensity", 0.0),
                        "timestamp": transcriptions[0].get("created_at") if transcriptions else None,
                    })
                
            except Exception as e:
                logger.warning("llm_fact_extraction_failed", error=str(e), fallback="basic")
        
        # Ð‘Ð°Ð·Ð¾Ð²Ð¾Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ (fallback Ð¸Ð»Ð¸ Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ)
        for trans in transcriptions:
            text = trans.get("text", "").strip()
            if not text:
                continue
            
            # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ: Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ
            sentences = [s.strip() for s in text.split(". ") if s.strip()]
            
            for i, sentence in enumerate(sentences):
                if len(sentence) > 20:  # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð° Ñ„Ð°ÐºÑ‚Ð°
                    facts.append({
                        "text": sentence,
                        "type": "fact",
                        "timestamp": trans.get("created_at"),
                        "source_id": trans.get("id"),
                        "confidence": trans.get("language_probability", 0.0),
                    })
        
        return facts
    
    def calculate_metrics(self, transcriptions: List[Dict], facts: List[Dict]) -> Dict:
        """
        Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð½Ñ.
        
        Returns:
            Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸
        """
        total_duration = sum(t.get("duration", 0) or 0 for t in transcriptions)
        total_chars = sum(len(t.get("text", "")) for t in transcriptions)
        total_words = sum(len(t.get("text", "").split()) for t in transcriptions)
        
        # Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ (ÑƒÐ¿Ñ€Ð¾Ñ‰Ñ‘Ð½Ð½Ð¾)
        # Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ = Ð¼Ð½Ð¾Ð³Ð¾ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð½Ð° ÐµÐ´Ð¸Ð½Ð¸Ñ†Ñƒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
        density_score = 0.0
        if total_duration > 0:
            facts_per_minute = (len(facts) / (total_duration / 60)) if total_duration > 0 else 0
            words_per_minute = (total_words / (total_duration / 60)) if total_duration > 0 else 0
            
            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ (Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°ÐµÐ¼ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚ÐµÐ¼Ð¿ ~150 ÑÐ»Ð¾Ð²/Ð¼Ð¸Ð½, ~5 Ñ„Ð°ÐºÑ‚Ð¾Ð²/Ð¼Ð¸Ð½)
            density_score = min(100, (facts_per_minute / 5) * 50 + (words_per_minute / 150) * 50)
        
        metrics = {
            "transcriptions_count": len(transcriptions),
            "facts_count": len(facts),
            "total_duration_minutes": round(total_duration / 60, 2) if total_duration else 0,
            "total_characters": total_chars,
            "total_words": total_words,
            "average_words_per_transcription": round(total_words / len(transcriptions), 1) if transcriptions else 0,
            "information_density_score": round(density_score, 1),
            "density_level": self._get_density_level(density_score),
        }
        
        return metrics
    
    def _get_density_level(self, score: float) -> str:
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚Ð¸."""
        if score >= 80:
            return "ðŸ”´ ÐžÑ‡ÐµÐ½ÑŒ Ð²Ñ‹ÑÐ¾ÐºÐ°Ñ"
        elif score >= 60:
            return "ðŸŸ  Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ"
        elif score >= 40:
            return "ðŸŸ¡ Ð¡Ñ€ÐµÐ´Ð½ÑÑ"
        elif score >= 20:
            return "ðŸŸ¢ ÐÐ¸Ð·ÐºÐ°Ñ"
        else:
            return "âšª ÐžÑ‡ÐµÐ½ÑŒ Ð½Ð¸Ð·ÐºÐ°Ñ"
    
    def generate_markdown(self, target_date: date, transcriptions: List[Dict], 
                         facts: List[Dict], metrics: Dict, include_metadata: bool = True) -> str:
        """
        Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ markdown-Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚.
        
        Args:
            target_date: Ð”Ð°Ñ‚Ð°
            transcriptions: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¹
            facts: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°ÐºÑ‚Ð¾Ð²
            metrics: ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
            include_metadata: Ð’ÐºÐ»ÑŽÑ‡Ð°Ñ‚ÑŒ Ð»Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
            
        Returns:
            Markdown Ñ‚ÐµÐºÑÑ‚
        """
        lines = []
        
        # Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº
        lines.append(f"# Reflexio Digest â€” {target_date.strftime('%d %B %Y')}")
        lines.append("")
        lines.append(f"*Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
        lines.append("")
        
        # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð½Ñ
        lines.append("## ðŸ“Š ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð½Ñ")
        lines.append("")
        lines.append(f"- **Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¹:** {metrics['transcriptions_count']}")
        lines.append(f"- **Ð¤Ð°ÐºÑ‚Ð¾Ð² Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾:** {metrics['facts_count']}")
        lines.append(f"- **ÐžÐ±Ñ‰Ð°Ñ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ:** {metrics['total_duration_minutes']} Ð¼Ð¸Ð½ÑƒÑ‚")
        lines.append(f"- **Ð¡Ð»Ð¾Ð² Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾:** {metrics['total_words']}")
        lines.append(f"- **Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ:** {metrics['information_density_score']}/100 ({metrics['density_level']})")
        
        # Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
        if 'extended_metrics' in metrics:
            from src.digest.metrics_ext import interpret_semantic_density, interpret_wpm_rate
            ext = metrics['extended_metrics']
            lines.append("")
            lines.append("### ðŸ§  ÐšÐ¾Ð³Ð½Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸")
            lines.append("")
            
            semantic_density = ext.get('semantic_density', 0)
            wpm = ext.get('wpm_rate', 0)
            
            lines.append(f"- **Ð¡ÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ:** {semantic_density:.3f}")
            lines.append(f"  *{interpret_semantic_density(semantic_density)}*")
            lines.append(f"- **Ð›ÐµÐºÑÐ¸Ñ‡ÐµÑÐºÐ¾Ðµ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ðµ:** {ext.get('lexical_diversity', 0):.3f}")
            lines.append(f"- **Ð¡ÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ Ñ€ÐµÑ‡Ð¸:** {wpm:.1f} ÑÐ»Ð¾Ð²/Ð¼Ð¸Ð½")
            lines.append(f"  *{interpret_wpm_rate(wpm)}*")
            lines.append(f"- **Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð´Ð»Ð¸Ð½Ð° ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°:** {ext.get('avg_words_per_segment', 0):.1f} ÑÐ»Ð¾Ð²")
            lines.append(f"- **Ð’Ð°Ñ€Ð¸Ð°Ñ†Ð¸Ñ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸:** {ext.get('hourly_variation', 0):.3f}")
            if 'segmentation' in ext:
                seg = ext['segmentation']
                lines.append(f"- **Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°:** {seg.get('avg_duration', 0):.1f} ÑÐµÐº")
        
        lines.append("")
        
        # Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ
        lines.append("### ðŸŽ¯ ÐÐ½Ð°Ð»Ð¸Ð· Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾ÑÑ‚Ð¸")
        lines.append("")
        
        density_desc = {
            "ðŸ”´ ÐžÑ‡ÐµÐ½ÑŒ Ð²Ñ‹ÑÐ¾ÐºÐ°Ñ": "ÐžÑ‡ÐµÐ½ÑŒ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ ÐºÐ¾Ð½Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸",
            "ðŸŸ  Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ": "Ð¥Ð¾Ñ€Ð¾ÑˆÐ¸Ð¹ Ð´ÐµÐ½ÑŒ Ñ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¼ Ð¾Ð±Ð¼ÐµÐ½Ð¾Ð¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹",
            "ðŸŸ¡ Ð¡Ñ€ÐµÐ´Ð½ÑÑ": "ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ ÑÐ¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð¾Ð¹ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒÑŽ",
            "ðŸŸ¢ ÐÐ¸Ð·ÐºÐ°Ñ": "Ð¡Ð¿Ð¾ÐºÐ¾Ð¹Ð½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ, Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°",
            "âšª ÐžÑ‡ÐµÐ½ÑŒ Ð½Ð¸Ð·ÐºÐ°Ñ": "ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ, Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ð°ÑƒÐ·Ð° Ð¸Ð»Ð¸ Ñ„Ð¾ÐºÑƒÑ Ð½Ð° Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…",
        }
        
        level = metrics['density_level']
        lines.append(f"**Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ:** {level}")
        lines.append("")
        lines.append(density_desc.get(level, "ÐÐµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½"))
        lines.append("")
        
        # Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð¾Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸ (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾)
        if SUMMARIZER_AVAILABLE and transcriptions:
            try:
                full_text = " ".join([t.get("text", "").strip() for t in transcriptions if t.get("text", "").strip()])
                if full_text:
                    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð»Ð¾Ñ‚Ð½Ð¾Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸ Ñ‡ÐµÑ€ÐµÐ· Chain of Density
                    dense_summary = generate_dense_summary(full_text, iterations=3)
                    
                    # Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· Critic
                    validated = validate_summary(
                        dense_summary["summary"],
                        original_text=full_text,
                        confidence_threshold=0.85,
                        auto_refine=True,
                    )
                    
                    lines.append("## ðŸ“‹ Ð”Ð½ÐµÐ²Ð½Ð¾Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸")
                    lines.append("")
                    lines.append(validated["summary"])
                    lines.append("")
                    
                    if validated.get("refined"):
                        lines.append(f"*Ð¡Ð°Ð¼Ð¼Ð°Ñ€Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¾ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ (confidence: {validated['confidence_score']:.2f})*")
                    else:
                        lines.append(f"*Confidence: {validated['confidence_score']:.2f}*")
                    lines.append("")
            except Exception as e:
                logger.warning("enhanced_summary_failed", error=str(e))
        
        # Ð¤Ð°ÐºÑ‚Ñ‹
        if facts:
            lines.append("## ðŸ“ Ð˜Ð·Ð²Ð»ÐµÑ‡Ñ‘Ð½Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹")
            lines.append("")
            for i, fact in enumerate(facts, 1):
                timestamp = fact.get("timestamp", "")[:16] if fact.get("timestamp") else ""
                fact_type = fact.get("type", "fact")
                lines.append(f"### {i}. [{fact_type.upper()}] {fact['text']}")
                if include_metadata and timestamp:
                    lines.append(f"*{timestamp}*")
                if fact_type == "task" and fact.get("priority"):
                    lines.append(f"*ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚: {fact['priority']}*")
                lines.append("")
        else:
            lines.append("## ðŸ“ Ð¤Ð°ÐºÑ‚Ñ‹")
            lines.append("")
            lines.append("*Ð¤Ð°ÐºÑ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹*")
            lines.append("")
        
        # Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹)
        if include_metadata and transcriptions:
            lines.append("## ðŸŽ¤ ÐŸÐ¾Ð»Ð½Ñ‹Ðµ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸")
            lines.append("")
            for i, trans in enumerate(transcriptions, 1):
                timestamp = trans.get("created_at", "")[:16] if trans.get("created_at") else ""
                language = trans.get("language", "unknown")
                duration = trans.get("duration", 0) or 0
                
                lines.append(f"### Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ #{i}")
                lines.append(f"*{timestamp} | {language} | {duration:.1f}s*")
                lines.append("")
                lines.append(f"> {trans.get('text', '')}")
                lines.append("")
        
        # ÐŸÐ¾Ð´Ð²Ð°Ð»
        lines.append("---")
        lines.append("")
        lines.append("*Reflexio 24/7 â€” Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð´Ð½ÐµÐ²Ð½Ð¾Ð¹ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚*")
        
        return "\n".join(lines)
    
    def generate_json(self, target_date: date, transcriptions: List[Dict],
                     facts: List[Dict], metrics: Dict) -> Dict:
        """
        Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ JSON-Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚ Ñ CoVe Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÐµÐ¹.
        
        Returns:
            Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ JSON-Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚
        """
        digest_dict = {
            "date": target_date.isoformat(),
            "generated_at": datetime.now().isoformat(),
            "metrics": metrics,
            "facts": facts,
            "transcriptions": transcriptions if transcriptions else [],
        }
        
        # CoVe Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚Ð°
        try:
            import sys
            import importlib.util
            from pathlib import Path as PathLib
            cove_path = PathLib(__file__).parent.parent.parent / ".cursor" / "validation" / "cove" / "verify.py"
            if cove_path.exists():
                spec = importlib.util.spec_from_file_location("cove_verify", cove_path)
                cove_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(cove_module)
                cove = cove_module.CoVeVerifier()
                
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ…ÐµÐ¼Ñƒ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚Ð°
                schema_valid, schema_errors = cove.verify_schema(digest_dict, "digest")
                if not schema_valid:
                    logger.warning("cove_digest_schema_validation_failed", errors=schema_errors)
                else:
                    logger.debug("cove_digest_validation_passed")
                
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ timestamps
                ts_valid, ts_errors = cove.verify_timestamps(digest_dict, ["generated_at"])
                if not ts_valid:
                    logger.warning("cove_digest_timestamps_validation_failed", errors=ts_errors)
        except Exception as e:
            logger.debug("cove_digest_validation_skipped", error=str(e))
        
        return digest_dict
    
    def generate(self, target_date: date, output_format: str = "markdown",
                include_metadata: bool = True, generate_pdf: bool = False) -> Path:
        """
        Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚ Ð´Ð»Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð¹ Ð´Ð°Ñ‚Ñ‹.
        
        Args:
            target_date: Ð”Ð°Ñ‚Ð°
            output_format: Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ ("markdown" Ð¸Ð»Ð¸ "json")
            include_metadata: Ð’ÐºÐ»ÑŽÑ‡Ð°Ñ‚ÑŒ Ð»Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
            
        Returns:
            ÐŸÑƒÑ‚ÑŒ Ðº ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ð¾Ð¼Ñƒ Ñ„Ð°Ð¹Ð»Ñƒ
        """
        logger.info("generating_digest", date=target_date.isoformat(), format=output_format)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
        transcriptions = self.get_transcriptions(target_date)
        
        if not transcriptions:
            logger.warning("no_transcriptions", date=target_date.isoformat())
            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚
            metrics = {
                "transcriptions_count": 0,
                "facts_count": 0,
                "total_duration_minutes": 0,
                "total_characters": 0,
                "total_words": 0,
                "average_words_per_transcription": 0,
                "information_density_score": 0.0,
                "density_level": "âšª ÐžÑ‡ÐµÐ½ÑŒ Ð½Ð¸Ð·ÐºÐ°Ñ",
            }
            facts = []
        else:
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ„Ð°ÐºÑ‚Ñ‹
            facts = self.extract_facts(transcriptions)
            
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
        metrics = self.calculate_metrics(transcriptions, facts)
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾)
        extended_enabled = getattr(settings, "EXTENDED_METRICS", False)
        if extended_enabled:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ‡Ð°ÑÐ°Ð¼ Ð´Ð»Ñ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº
            hourly_dist = {}
            for trans in transcriptions:
                if trans.get("created_at"):
                    try:
                        hour = datetime.fromisoformat(trans["created_at"]).strftime("%H")
                        hourly_dist[hour] = hourly_dist.get(hour, 0) + 1
                    except:
                        pass
            
            extended = calculate_extended_metrics(
                transcriptions=transcriptions,
                hourly_distribution=hourly_dist,
                enabled=True,
            )
            if extended:
                metrics["extended_metrics"] = extended
        
        # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚
        if output_format == "pdf" or generate_pdf:
            try:
                from src.digest.pdf_generator import PDFGenerator
                pdf_gen = PDFGenerator()
                output_file = pdf_gen.generate(
                    target_date=target_date,
                    transcriptions=transcriptions,
                    facts=facts,
                    metrics=metrics,
                )
                logger.info(
                    "digest_pdf_generated",
                    date=target_date.isoformat(),
                    file=str(output_file),
                )
                return output_file
            except ImportError:
                logger.warning("pdf_generation_failed", reason="reportlab_not_available", fallback="markdown")
                # Fallback Ð½Ð° markdown
                output_format = "markdown"
        
        if output_format == "json":
            content = json.dumps(self.generate_json(target_date, transcriptions, facts, metrics), 
                               indent=2, ensure_ascii=False)
            ext = "json"
        else:
            content = self.generate_markdown(target_date, transcriptions, facts, metrics, include_metadata)
            ext = "md"
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼
        output_file = self.digests_dir / f"digest_{target_date.isoformat()}.{ext}"
        output_file.write_text(content, encoding="utf-8")
        
        logger.info(
            "digest_generated",
            date=target_date.isoformat(),
            file=str(output_file),
            facts=len(facts),
            transcriptions=len(transcriptions),
            density=metrics.get("information_density_score", 0),
        )
        
        # Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ñ Ð´Ð°Ð¹Ð´Ð¶ÐµÑÑ‚Ð¾Ð¼ (Ð¸Ð½ÑÐ°Ð¹Ñ‚Ñ‹ Ð¸ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹)
        try:
            core_memory = get_core_memory()
            session_memory = get_session_memory()
            
            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÐµÑÑÐ¸ÑŽ Ð´Ð»Ñ Ð´Ð½Ñ
            session_id = f"day_{target_date.isoformat()}"
            session_memory.create_session(session_id, metadata={
                "date": target_date.isoformat(),
                "transcriptions_count": len(transcriptions),
                "facts_count": len(facts),
                "density_score": metrics.get("information_density_score", 0),
            })
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ñ‹ Ð¸Ð· Ñ„Ð°ÐºÑ‚Ð¾Ð²
            for fact in facts[:20]:  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾
                session_memory.add_context(session_id, {
                    "type": fact.get("type", "fact"),
                    "text": fact.get("text", ""),
                    "timestamp": fact.get("timestamp"),
                })
            
            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ core memory Ñ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð°Ð¼Ð¸
            if facts:
                # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð° Ñ‚Ð¸Ð¿Ð¾Ð² Ñ„Ð°ÐºÑ‚Ð¾Ð²)
                fact_types = {}
                for fact in facts:
                    fact_type = fact.get("type", "fact")
                    fact_types[fact_type] = fact_types.get(fact_type, 0) + 1
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð½Ñ
                daily_patterns = core_memory.get("daily_patterns", {})
                daily_patterns[target_date.isoformat()] = {
                    "fact_types": fact_types,
                    "density_score": metrics.get("information_density_score", 0),
                }
                # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ (Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 30 Ð´Ð½ÐµÐ¹)
                if len(daily_patterns) > 30:
                    oldest_date = min(daily_patterns.keys())
                    del daily_patterns[oldest_date]
                core_memory.set("daily_patterns", daily_patterns)
            
            logger.info("memory_synced_with_digest", date=target_date.isoformat())
            
        except Exception as e:
            logger.warning("memory_sync_failed", error=str(e), continue_without_sync=True)
        
        return output_file

