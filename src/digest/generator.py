"""–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤ –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º summarization."""
import sqlite3
from datetime import date, datetime
from pathlib import Path
from typing import Optional, Dict, List
import json

from src.utils.config import settings
from src.utils.logging import setup_logging, get_logger
from src.digest.metrics_ext import calculate_extended_metrics
from src.storage.ingest_persist import ensure_ingest_tables
from src.memory.core_memory import get_core_memory
from src.memory.session_memory import get_session_memory

# –ù–æ–≤—ã–µ –º–æ–¥—É–ª–∏ summarization (November 2025 Integration Sprint)
try:
    from src.summarizer.chain_of_density import generate_dense_summary
    from src.summarizer.critic import validate_summary
    from src.summarizer.few_shot import extract_tasks, analyze_emotions
    SUMMARIZER_AVAILABLE = True
except ImportError:
    SUMMARIZER_AVAILABLE = False
    logger = get_logger("digest")
    logger.warning("summarizer_modules_not_available", using_basic_summary=True)

setup_logging()
logger = get_logger("digest")


class DigestGenerator:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç –¥–Ω—è –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤."""

    # –ü–û–ß–ï–ú–£ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: Whisper —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π —à—É–º –∫–∞–∫ "you", "the", –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏.
    # 10 000+ –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π ‚Üí LLM –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É–µ—Ç –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–æ–º/–≥—Ä—É–∑–∏–Ω—Å–∫–æ–º.
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ LLM: –º–∏–Ω–∏–º—É–º 3 —Å–ª–æ–≤–∞, –Ω–µ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã, language_probability > 0.4.
    NOISE_PHRASES = frozenset({
        "you", "the", "a", "an", "i", "he", "she", "it", "we", "they",
        "yes", "no", "oh", "ah", "um", "uh", "hmm", "huh",
        "that's it", "thank you", "thanks", "okay", "ok",
        "—É–≥—É", "–∞–≥–∞", "–Ω—É", "–º–º", "—Ö–º", "—ç—Ç–æ", "–ª–∞–¥–Ω–æ", "–ø–æ–Ω—è–ª", "–æ–∫–µ–π",
    })
    MIN_WORDS = 3
    MIN_LANG_PROBABILITY = 0.4
    MAX_TRANSCRIPTIONS_FOR_LLM = 100  # –õ–∏–º–∏—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è LLM-–∑–∞–¥–∞—á (extract_facts)
    MAX_TEXT_LENGTH_FOR_LLM = 16000  # Tiered CoD –≤—Ö–æ–¥ –¥–ª—è –¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞
    MAX_HOURLY_CHUNK_CHARS = 6000

    def __init__(self, db_path: Optional[Path] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞."""
        if db_path is None:
            db_path = settings.STORAGE_PATH / "reflexio.db"
        self.db_path = db_path
        self.digests_dir = Path("digests")
        self.digests_dir.mkdir(parents=True, exist_ok=True)

    @staticmethod
    def _is_meaningful(text: str, lang_prob: float = 1.0) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç."""
        text = text.strip()
        if not text:
            return False
        words = text.split()
        if len(words) < DigestGenerator.MIN_WORDS:
            return False
        if text.lower() in DigestGenerator.NOISE_PHRASES:
            return False
        if lang_prob < DigestGenerator.MIN_LANG_PROBABILITY:
            return False
        return True

    @staticmethod
    def _has_cyrillic(text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É (—Ä—É—Å—Å–∫–∏–π)."""
        return any('\u0400' <= c <= '\u04ff' for c in text)

    @staticmethod
    def _hour_bucket(iso_ts: str | None) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç timestamp –≤ –±–∞–∫–µ—Ç YYYY-MM-DD HH:00."""
        if not iso_ts:
            return "unknown"
        try:
            dt = datetime.fromisoformat(str(iso_ts).replace("Z", "+00:00"))
            return dt.strftime("%Y-%m-%d %H:00")
        except Exception:
            return "unknown"

    def _iter_meaningful_texts(self, transcriptions: List[Dict]) -> List[str]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±–µ–∑ –æ–±—Ä–µ–∑–∫–∏ –ø–æ –¥–ª–∏–Ω–µ."""
        meaningful: List[str] = []
        for t in transcriptions:
            text = (t.get("text") or "").strip()
            lang_prob = t.get("language_probability") or 1.0
            if self._is_meaningful(text, lang_prob):
                meaningful.append(text)
        return meaningful

    def _join_meaningful_text(
        self,
        transcriptions: List[Dict],
        max_transcriptions: Optional[int],
        max_chars: Optional[int],
    ) -> str:
        """–°–æ–±–∏—Ä–∞–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏."""
        meaningful = self._iter_meaningful_texts(transcriptions)
        if max_transcriptions:
            meaningful = meaningful[-max_transcriptions:]
        joined = " ".join(meaningful)
        if max_chars and len(joined) > max_chars:
            joined = joined[-max_chars:]
        return joined

    def _get_meaningful_text(self, transcriptions: List[Dict]) -> str:
        """–°–æ–±–∏—Ä–∞–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è LLM-–∑–∞–¥–∞—á —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –±—é–¥–∂–µ—Ç–∞."""
        return self._join_meaningful_text(
            transcriptions,
            max_transcriptions=self.MAX_TRANSCRIPTIONS_FOR_LLM,
            max_chars=self.MAX_TEXT_LENGTH_FOR_LLM,
        )

    def _build_tiered_digest_input(self, transcriptions: List[Dict]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –≤—Ö–æ–¥ –¥–ª—è –¥–Ω–µ–≤–Ω–æ–≥–æ CoD: –ª–∏–±–æ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç, –ª–∏–±–æ summary-of-summaries –ø–æ —á–∞—Å–∞–º."""
        full_text = self._join_meaningful_text(
            transcriptions,
            max_transcriptions=None,
            max_chars=None,
        )
        if not full_text:
            return ""

        if len(full_text) <= self.MAX_TEXT_LENGTH_FOR_LLM:
            return full_text

        # Fallback –±–µ–∑ summarizer: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º—Å—è —Ö–≤–æ—Å—Ç–æ–º –¥–Ω—è.
        if not SUMMARIZER_AVAILABLE:
            return full_text[-self.MAX_TEXT_LENGTH_FOR_LLM:]

        grouped: Dict[str, List[Dict]] = {}
        for t in transcriptions:
            bucket = self._hour_bucket(t.get("created_at"))
            grouped.setdefault(bucket, []).append(t)

        hourly_summaries: List[str] = []
        for hour in sorted(grouped.keys()):
            chunk_text = self._join_meaningful_text(
                grouped[hour],
                max_transcriptions=None,
                max_chars=self.MAX_HOURLY_CHUNK_CHARS,
            )
            if not chunk_text:
                continue
            try:
                dense = generate_dense_summary(chunk_text, iterations=1)
                summary = (dense.get("summary") or "").strip()
            except Exception as e:
                logger.warning("hourly_cod_failed", hour=hour, error=str(e))
                summary = ""

            if not summary:
                summary = chunk_text[:1000]
            hourly_summaries.append(f"[{hour}] {summary}")

        if not hourly_summaries:
            return full_text[-self.MAX_TEXT_LENGTH_FOR_LLM:]

        tiered = "\n".join(hourly_summaries)
        if len(tiered) > self.MAX_TEXT_LENGTH_FOR_LLM:
            tiered = tiered[-self.MAX_TEXT_LENGTH_FOR_LLM:]
        return tiered

    def get_transcriptions(self, target_date: date) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∑–∞ –¥–µ–Ω—å.
        
        Args:
            target_date: –î–∞—Ç–∞ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self.db_path.parent.exists():
            self.db_path.parent.mkdir(parents=True, exist_ok=True)
        ensure_ingest_tables(self.db_path)
        if not self.db_path.exists():
            logger.warning("database_not_found", db_path=str(self.db_path))
            return []
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row
        
        try:
            cursor = conn.cursor()

            # –ü–û–ß–ï–ú–£ is_user —Ñ–∏–ª—å—Ç—Ä: –ø–æ—Å–ª–µ –≤–∫–ª—é—á–µ–Ω–∏—è Speaker Verification –¥–∞–π–¥–∂–µ—Å—Ç
            # –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–µ —Ñ–æ–Ω–æ–≤—ã–π TV/—Ä–∞–¥–∏–æ.
            # DEFAULT 1 –≤ —Å—Ö–µ–º–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç backward-compatibility (—Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ = user).
            # –ö–æ–≥–¥–∞ SPEAKER_VERIFICATION_ENABLED=False ‚Äî –≤—Å—ë is_user=1, —Ñ–∏–ª—å—Ç—Ä –±–µ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∞.
            speaker_filter = (
                "AND (t.is_user = 1 OR t.is_user IS NULL)"
                if settings.SPEAKER_VERIFICATION_ENABLED
                else ""
            )

            # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∑–∞ –¥–µ–Ω—å
            cursor.execute(f"""
                SELECT
                    t.id,
                    t.ingest_id,
                    t.text,
                    t.language,
                    t.language_probability,
                    t.duration,
                    t.segments,
                    t.created_at,
                    i.filename,
                    i.file_size
                FROM transcriptions t
                LEFT JOIN ingest_queue i ON t.ingest_id = i.id
                WHERE DATE(t.created_at) = ? {speaker_filter}
                ORDER BY t.created_at ASC
            """, (target_date.isoformat(),))  # nosec B608 ‚Äî speaker_filter is a hardcoded literal string, date is parameterized
            
            rows = cursor.fetchall()
            transcriptions = [dict(row) for row in rows]
            
            logger.info(
                "transcriptions_found",
                date=target_date.isoformat(),
                count=len(transcriptions),
            )
            
            return transcriptions
            
        finally:
            conn.close()
    
    def extract_facts(self, transcriptions: List[Dict], use_llm: bool = True) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç—ã –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º LLM-–∞–Ω–∞–ª–∏–∑–æ–º.
        
        Args:
            transcriptions: –°–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π
            use_llm: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        """
        facts = []
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ Whisper)
        full_text = self._get_meaningful_text(transcriptions)
        
        if not full_text:
            return facts
        
        # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω summarizer, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if use_llm and SUMMARIZER_AVAILABLE:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ few-shot
                tasks = extract_tasks(full_text)
                for task in tasks:
                    facts.append({
                        "text": task.get("task", ""),
                        "type": "task",
                        "priority": task.get("priority", "medium"),
                        "deadline": task.get("deadline"),
                        "timestamp": transcriptions[0].get("created_at") if transcriptions else None,
                    })
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–∏
                emotions = analyze_emotions(full_text)
                if emotions.get("emotions"):
                    facts.append({
                        "text": f"–≠–º–æ—Ü–∏–∏: {', '.join(emotions.get('emotions', []))}",
                        "type": "emotion",
                        "intensity": emotions.get("intensity", 0.0),
                        "timestamp": transcriptions[0].get("created_at") if transcriptions else None,
                    })
                
            except Exception as e:
                logger.warning("llm_fact_extraction_failed", error=str(e), fallback="basic")
        
        # –ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ (fallback –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ) ‚Äî —Ç–æ–ª—å–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        for trans in transcriptions:
            text = (trans.get("text") or "").strip()
            lang_prob = trans.get("language_probability") or 1.0
            if not self._is_meaningful(text, lang_prob):
                continue

            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = [s.strip() for s in text.split(". ") if s.strip()]

            for i, sentence in enumerate(sentences):
                if len(sentence) > 20 and self._has_cyrillic(sentence):
                    facts.append({
                        "text": sentence,
                        "type": "fact",
                        "timestamp": trans.get("created_at"),
                        "source_id": trans.get("id"),
                        "confidence": lang_prob,
                    })
        
        return facts
    
    def calculate_metrics(self, transcriptions: List[Dict], facts: List[Dict]) -> Dict:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–Ω—è.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        total_duration = sum(t.get("duration", 0) or 0 for t in transcriptions)
        total_chars = sum(len(t.get("text", "")) for t in transcriptions)
        total_words = sum(len(t.get("text", "").split()) for t in transcriptions)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
        # –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å = –º–Ω–æ–≥–æ —Ñ–∞–∫—Ç–æ–≤ –Ω–∞ –µ–¥–∏–Ω–∏—Ü—É –≤—Ä–µ–º–µ–Ω–∏
        density_score = 0.0
        if total_duration > 0:
            facts_per_minute = (len(facts) / (total_duration / 60)) if total_duration > 0 else 0
            words_per_minute = (total_words / (total_duration / 60)) if total_duration > 0 else 0
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–º–ø ~150 —Å–ª–æ–≤/–º–∏–Ω, ~5 —Ñ–∞–∫—Ç–æ–≤/–º–∏–Ω)
            density_score = min(100, (facts_per_minute / 5) * 50 + (words_per_minute / 150) * 50)
        
        metrics = {
            "transcriptions_count": len(transcriptions),
            "facts_count": len(facts),
            "total_duration_minutes": round(total_duration / 60, 2) if total_duration else 0,
            "total_characters": total_chars,
            "total_words": total_words,
            "average_words_per_transcription": round(total_words / len(transcriptions), 1) if transcriptions else 0,
            "information_density_score": round(density_score, 1),
            "density_level": self._get_density_level(density_score),
        }
        
        return metrics
    
    def _get_recording_analyses_for_date(self, target_date: date) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–Ω–∞–ª–∏–∑—ã –∑–∞–ø–∏—Å–µ–π (recording_analyses) –∑–∞ –¥–µ–Ω—å –ø–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è–º."""
        if not self.db_path.exists():
            return []
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row
        try:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT ra.id, ra.transcription_id, ra.summary, ra.emotions, ra.actions, ra.topics, ra.urgency
                FROM recording_analyses ra
                INNER JOIN transcriptions t ON ra.transcription_id = t.id
                WHERE DATE(t.created_at) = ?
                ORDER BY t.created_at ASC
            """, (target_date.isoformat(),))
            rows = cursor.fetchall()
            out = []
            for row in rows:
                d = dict(row)
                for key in ("emotions", "actions", "topics"):
                    if isinstance(d.get(key), str):
                        try:
                            d[key] = json.loads(d[key]) if d[key] else []
                        except (json.JSONDecodeError, TypeError):
                            d[key] = []
                out.append(d)
            return out
        except sqlite3.OperationalError as e:
            if "no such table" in str(e).lower():
                return []
            raise
        finally:
            conn.close()

    def get_daily_digest_json(self, target_date: date, user_id: Optional[str] = None) -> Dict:
        """
        –î–Ω–µ–≤–Ω–æ–π –∏—Ç–æ–≥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ API –¥–ª—è Android (ROADMAP Phase 2).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: date, summary_text, key_themes, emotions, actions, total_recordings, total_duration, repetitions.
        """
        transcriptions = self.get_transcriptions(target_date)
        analyses = self._get_recording_analyses_for_date(target_date)
        total_duration_sec = sum(t.get("duration", 0) or 0 for t in transcriptions)
        total_recordings = len(transcriptions)
        total_duration_str = f"{int(total_duration_sec // 60)}m {int(total_duration_sec % 60)}s" if total_duration_sec else "0m 0s"

        key_themes: List[str] = []
        emotions: List[str] = []
        actions: List[Dict] = []
        summary_parts: List[str] = []

        if analyses:
            for a in analyses:
                part = (a.get("summary") or "").strip()
                # –ü–û–ß–ï–ú–£ —Ñ–∏–ª—å—Ç—Ä –∫–∏—Ä–∏–ª–ª–∏—Ü—ã: —Å—Ç–∞—Ä—ã–µ analyses —Å–æ–¥–µ—Ä–∂–∞—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
                # –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–æ–º/–≥—Ä—É–∑–∏–Ω—Å–∫–æ–º –∏–∑-–∑–∞ –º—É—Å–æ—Ä–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π.
                if part and self._has_cyrillic(part):
                    summary_parts.append(part)
                for t in (a.get("topics") or []):
                    if t and t not in key_themes and self._has_cyrillic(t):
                        key_themes.append(t)
                for e in (a.get("emotions") or []):
                    if e and e not in emotions:
                        emotions.append(e)
                for act in (a.get("actions") or []):
                    if isinstance(act, str) and act and self._has_cyrillic(act):
                        actions.append({"text": act, "done": False})
                    elif isinstance(act, dict) and act.get("text") and self._has_cyrillic(act["text"]):
                        actions.append({"text": act["text"], "done": act.get("done", False)})
            summary_text = " ".join(p for p in summary_parts if p).strip() or "–ù–µ—Ç –∏—Ç–æ–≥–∞ –∑–∞ –¥–µ–Ω—å."
        else:
            facts = self.extract_facts(transcriptions)
            for f in facts:
                if f.get("type") == "task":
                    actions.append({"text": f.get("text", ""), "done": False})
                elif f.get("type") == "emotion":
                    emo_text = f.get("text", "").replace("–≠–º–æ—Ü–∏–∏: ", "")
                    for e in emo_text.split(","):
                        e = e.strip()
                        if e and e not in emotions:
                            emotions.append(e)
            # –ü–û–ß–ï–ú–£ _has_cyrillic: –±–µ–∑ —ç—Ç–æ–≥–æ key_themes —Å–æ–¥–µ—Ä–∂–∏—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ Whisper –Ω–∞ –≤—Å–µ—Ö —è–∑—ã–∫–∞—Ö
            key_themes = list(dict.fromkeys(
                f.get("text", "").split(":")[-1].strip()
                for f in facts
                if f.get("type") == "fact" and len((f.get("text") or "")) > 15
                and self._has_cyrillic(f.get("text", ""))
            ))[:15]
            full_text = self._build_tiered_digest_input(transcriptions)
            # –ü–û–ß–ï–ú–£ CoD –∑–¥–µ—Å—å: –±–µ–∑ LLM-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ summary_text = —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç
            # —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π ("–Ø –ø–æ–µ–¥—É –∫—É–ø–ª—é..."), –∞ –Ω–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –∏—Ç–æ–≥ –¥–Ω—è.
            if full_text and SUMMARIZER_AVAILABLE:
                try:
                    dense = generate_dense_summary(full_text, iterations=2)
                    validated = validate_summary(
                        dense["summary"],
                        original_text=full_text,
                        confidence_threshold=0.7,
                        auto_refine=False,
                    )
                    summary_text = validated["summary"] or full_text[:500]
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–æ—Ü–∏–∏ –∏ –∑–∞–¥–∞—á–∏ –∏–∑ LLM
                    try:
                        tasks = extract_tasks(full_text)
                        for task in tasks:
                            t = task.get("task", "")
                            if t and self._has_cyrillic(t):
                                actions.append({"text": t, "done": False})
                        emo = analyze_emotions(full_text)
                        for e in (emo.get("emotions") or []):
                            if e and e not in emotions:
                                emotions.append(e)
                    except Exception:
                        pass
                except Exception as e:
                    logger.warning("cod_digest_failed", error=str(e))
                    summary_text = full_text[:500] + "‚Ä¶" if len(full_text) > 500 else full_text or "–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –∑–∞ –¥–µ–Ω—å."
            else:
                summary_text = full_text[:500] + "‚Ä¶" if len(full_text) > 500 else full_text or "–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –∑–∞ –¥–µ–Ω—å."

        balance_payload = {}
        insights = []
        try:
            from src.balance.storage import get_balance_wheel
            balance_payload = get_balance_wheel(self.db_path, target_date, target_date)
        except Exception:
            balance_payload = {}

        try:
            from src.persongraph.service import save_day_psychology_snapshot, get_day_insights
            day_text = self._build_tiered_digest_input(transcriptions)
            if day_text:
                save_day_psychology_snapshot(self.db_path, target_date.isoformat(), day_text)
            insights = get_day_insights(self.db_path, target_date.isoformat())
        except Exception:
            insights = []

        return {
            "date": target_date.isoformat(),
            "summary_text": summary_text,
            "key_themes": key_themes,
            "emotions": emotions,
            "actions": actions,
            "total_recordings": total_recordings,
            "total_duration": total_duration_str,
            "repetitions": [],
            "balance": balance_payload,
            "insights": insights,
        }

    def _get_density_level(self, score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏."""
        if score >= 80:
            return "üî¥ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
        elif score >= 60:
            return "üü† –í—ã—Å–æ–∫–∞—è"
        elif score >= 40:
            return "üü° –°—Ä–µ–¥–Ω—è—è"
        elif score >= 20:
            return "üü¢ –ù–∏–∑–∫–∞—è"
        else:
            return "‚ö™ –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"
    
    def generate_markdown(self, target_date: date, transcriptions: List[Dict], 
                         facts: List[Dict], metrics: Dict, include_metadata: bool = True) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç markdown-–¥–∞–π–¥–∂–µ—Å—Ç.
        
        Args:
            target_date: –î–∞—Ç–∞
            transcriptions: –°–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π
            facts: –°–ø–∏—Å–æ–∫ —Ñ–∞–∫—Ç–æ–≤
            metrics: –ú–µ—Ç—Ä–∏–∫–∏
            include_metadata: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            
        Returns:
            Markdown —Ç–µ–∫—Å—Ç
        """
        lines = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        lines.append(f"# Reflexio Digest ‚Äî {target_date.strftime('%d %B %Y')}")
        lines.append("")
        lines.append(f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
        lines.append("")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–Ω—è
        lines.append("## üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–Ω—è")
        lines.append("")
        lines.append(f"- **–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π:** {metrics['transcriptions_count']}")
        lines.append(f"- **–§–∞–∫—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–æ:** {metrics['facts_count']}")
        lines.append(f"- **–û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {metrics['total_duration_minutes']} –º–∏–Ω—É—Ç")
        lines.append(f"- **–°–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ:** {metrics['total_words']}")
        lines.append(f"- **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å:** {metrics['information_density_score']}/100 ({metrics['density_level']})")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if 'extended_metrics' in metrics:
            from src.digest.metrics_ext import interpret_semantic_density, interpret_wpm_rate
            ext = metrics['extended_metrics']
            lines.append("")
            lines.append("### üß† –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
            lines.append("")
            
            semantic_density = ext.get('semantic_density', 0)
            wpm = ext.get('wpm_rate', 0)
            
            lines.append(f"- **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å:** {semantic_density:.3f}")
            lines.append(f"  *{interpret_semantic_density(semantic_density)}*")
            lines.append(f"- **–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ:** {ext.get('lexical_diversity', 0):.3f}")
            lines.append(f"- **–°–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏:** {wpm:.1f} —Å–ª–æ–≤/–º–∏–Ω")
            lines.append(f"  *{interpret_wpm_rate(wpm)}*")
            lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞:** {ext.get('avg_words_per_segment', 0):.1f} —Å–ª–æ–≤")
            lines.append(f"- **–í–∞—Ä–∏–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:** {ext.get('hourly_variation', 0):.3f}")
            if 'segmentation' in ext:
                seg = ext['segmentation']
                lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞:** {seg.get('avg_duration', 0):.1f} —Å–µ–∫")
        
        lines.append("")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        lines.append("### üéØ –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏")
        lines.append("")
        
        density_desc = {
            "üî¥ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è": "–û—á–µ–Ω—å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–π –¥–µ–Ω—å —Å –≤—ã—Å–æ–∫–æ–π –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "üü† –í—ã—Å–æ–∫–∞—è": "–•–æ—Ä–æ—à–∏–π –¥–µ–Ω—å —Å –∞–∫—Ç–∏–≤–Ω—ã–º –æ–±–º–µ–Ω–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π",
            "üü° –°—Ä–µ–¥–Ω—è—è": "–û–±—ã—á–Ω—ã–π –¥–µ–Ω—å —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é",
            "üü¢ –ù–∏–∑–∫–∞—è": "–°–ø–æ–∫–æ–π–Ω—ã–π –¥–µ–Ω—å, –º–µ–Ω—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞",
            "‚ö™ –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è": "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –≤–æ–∑–º–æ–∂–Ω–æ –ø–∞—É–∑–∞ –∏–ª–∏ —Ñ–æ–∫—É—Å –Ω–∞ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö",
        }
        
        level = metrics['density_level']
        lines.append(f"**–£—Ä–æ–≤–µ–Ω—å:** {level}")
        lines.append("")
        lines.append(density_desc.get(level, "–ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω"))
        lines.append("")

        daily_payload = {}
        try:
            daily_payload = self.get_daily_digest_json(target_date)
        except Exception:
            daily_payload = {}

        balance = daily_payload.get("balance", {}) if isinstance(daily_payload, dict) else {}
        insights = daily_payload.get("insights", []) if isinstance(daily_payload, dict) else []

        if balance.get("domains"):
            lines.append("## ‚öñÔ∏è –ö–æ–ª–µ—Å–æ –ë–∞–ª–∞–Ω—Å–∞")
            lines.append("")
            for domain in balance.get("domains", []):
                lines.append(
                    f"- **{domain.get('domain', 'domain')}**: {domain.get('score', 0)}/10, "
                    f"—É–ø–æ–º–∏–Ω–∞–Ω–∏–π {domain.get('mentions', 0)}, sentiment {domain.get('sentiment', 0)}"
                )
            if balance.get("alert"):
                lines.append("")
                lines.append(f"‚ö†Ô∏è {balance.get('alert')}")
            if balance.get("recommendation"):
                lines.append(f"üí° {balance.get('recommendation')}")
            lines.append("")

        if insights:
            lines.append("## üß≠ –ò–Ω—Å–∞–π—Ç—ã")
            lines.append("")
            for insight in insights:
                role = insight.get("role", "analyst")
                text = insight.get("insight", "")
                if text:
                    lines.append(f"- **{role}:** {text}")
            lines.append("")

        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        if SUMMARIZER_AVAILABLE and transcriptions:
            try:
                full_text = self._build_tiered_digest_input(transcriptions)
                if full_text:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–ª–æ—Ç–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ —á–µ—Ä–µ–∑ Chain of Density
                    dense_summary = generate_dense_summary(full_text, iterations=3)
                    
                    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ Critic
                    # –ü–û–ß–ï–ú–£ auto_refine=False: —Å 0.85 –ø–æ—Ä–æ–≥–æ–º –∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º–∏
                    # confidence –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ < 0.85 ‚Üí –ª–∏—à–Ω–∏–π LLM –≤—ã–∑–æ–≤ –∫–∞–∂–¥—ã–π —Ä–∞–∑.
                    # 0.70 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç get_daily_digest_json. False = –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å —Ç–æ–∫–µ–Ω—ã.
                    validated = validate_summary(
                        dense_summary["summary"],
                        original_text=full_text,
                        confidence_threshold=0.70,
                        auto_refine=False,
                    )
                    
                    lines.append("## üìã –î–Ω–µ–≤–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏")
                    lines.append("")
                    lines.append(validated["summary"])
                    lines.append("")
                    
                    if validated.get("refined"):
                        lines.append(f"*–°–∞–º–º–∞—Ä–∏ —É–ª—É—á—à–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ (confidence: {validated['confidence_score']:.2f})*")
                    else:
                        lines.append(f"*Confidence: {validated['confidence_score']:.2f}*")
                    lines.append("")
            except Exception as e:
                logger.warning("enhanced_summary_failed", error=str(e))
        
        # –§–∞–∫—Ç—ã
        if facts:
            lines.append("## üìù –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã")
            lines.append("")
            for i, fact in enumerate(facts, 1):
                timestamp = fact.get("timestamp", "")[:16] if fact.get("timestamp") else ""
                fact_type = fact.get("type", "fact")
                lines.append(f"### {i}. [{fact_type.upper()}] {fact['text']}")
                if include_metadata and timestamp:
                    lines.append(f"*{timestamp}*")
                if fact_type == "task" and fact.get("priority"):
                    lines.append(f"*–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {fact['priority']}*")
                lines.append("")
        else:
            lines.append("## üìù –§–∞–∫—Ç—ã")
            lines.append("")
            lines.append("*–§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã*")
            lines.append("")
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã)
        if include_metadata and transcriptions:
            lines.append("## üé§ –ü–æ–ª–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
            lines.append("")
            for i, trans in enumerate(transcriptions, 1):
                timestamp = trans.get("created_at", "")[:16] if trans.get("created_at") else ""
                language = trans.get("language", "unknown")
                duration = trans.get("duration", 0) or 0
                
                lines.append(f"### –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è #{i}")
                lines.append(f"*{timestamp} | {language} | {duration:.1f}s*")
                lines.append("")
                lines.append(f"> {trans.get('text', '')}")
                lines.append("")
        
        # –ü–æ–¥–≤–∞–ª
        lines.append("---")
        lines.append("")
        lines.append("*Reflexio 24/7 ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–Ω–µ–≤–Ω–æ–π –¥–∞–π–¥–∂–µ—Å—Ç*")
        
        return "\n".join(lines)
    
    def generate_json(self, target_date: date, transcriptions: List[Dict],
                     facts: List[Dict], metrics: Dict) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç JSON-–¥–∞–π–¥–∂–µ—Å—Ç —Å CoVe –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        
        Returns:
            –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON-–¥–∞–π–¥–∂–µ—Å—Ç
        """
        digest_dict = {
            "date": target_date.isoformat(),
            "generated_at": datetime.now().isoformat(),
            "metrics": metrics,
            "facts": facts,
            "transcriptions": transcriptions if transcriptions else [],
        }
        
        # –ü–û–ß–ï–ú–£ —É–±—Ä–∞–ª–∏ CoVe importlib: –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ H5 safe_middleware ‚Äî
        # exec_module() –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ .py —Ñ–∞–π–ª–∞ –∏–∑ .cursor/ = RCE –≤–µ–∫—Ç–æ—Ä.
        return digest_dict
    
    def generate(self, target_date: date, output_format: str = "markdown",
                include_metadata: bool = True, generate_pdf: bool = False) -> Path:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã.
        
        Args:
            target_date: –î–∞—Ç–∞
            output_format: –§–æ—Ä–º–∞—Ç ("markdown" –∏–ª–∏ "json")
            include_metadata: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        logger.info("generating_digest", date=target_date.isoformat(), format=output_format)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        transcriptions = self.get_transcriptions(target_date)
        
        if not transcriptions:
            logger.warning("no_transcriptions", date=target_date.isoformat())
            # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π –¥–∞–π–¥–∂–µ—Å—Ç
            metrics = {
                "transcriptions_count": 0,
                "facts_count": 0,
                "total_duration_minutes": 0,
                "total_characters": 0,
                "total_words": 0,
                "average_words_per_transcription": 0,
                "information_density_score": 0.0,
                "density_level": "‚ö™ –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è",
            }
            facts = []
        else:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã
            facts = self.extract_facts(transcriptions)
            
        # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics = self.calculate_metrics(transcriptions, facts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        extended_enabled = getattr(settings, "EXTENDED_METRICS", False)
        if extended_enabled:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —á–∞—Å–∞–º –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            hourly_dist = {}
            for trans in transcriptions:
                if trans.get("created_at"):
                    try:
                        hour = datetime.fromisoformat(trans["created_at"]).strftime("%H")
                        hourly_dist[hour] = hourly_dist.get(hour, 0) + 1
                    except Exception:
                        pass
            
            extended = calculate_extended_metrics(
                transcriptions=transcriptions,
                hourly_distribution=hourly_dist,
                enabled=True,
            )
            if extended:
                metrics["extended_metrics"] = extended
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–π–¥–∂–µ—Å—Ç
        if output_format == "pdf" or generate_pdf:
            try:
                from src.digest.pdf_generator import PDFGenerator
                pdf_gen = PDFGenerator()
                output_file = pdf_gen.generate(
                    target_date=target_date,
                    transcriptions=transcriptions,
                    facts=facts,
                    metrics=metrics,
                )
                logger.info(
                    "digest_pdf_generated",
                    date=target_date.isoformat(),
                    file=str(output_file),
                )
                return output_file
            except ImportError:
                logger.warning("pdf_generation_failed", reason="reportlab_not_available", fallback="markdown")
                # Fallback –Ω–∞ markdown
                output_format = "markdown"
        
        if output_format == "json":
            content = json.dumps(self.generate_json(target_date, transcriptions, facts, metrics), 
                               indent=2, ensure_ascii=False)
            ext = "json"
        else:
            content = self.generate_markdown(target_date, transcriptions, facts, metrics, include_metadata)
            ext = "md"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        output_file = self.digests_dir / f"digest_{target_date.isoformat()}.{ext}"
        output_file.write_text(content, encoding="utf-8")
        
        logger.info(
            "digest_generated",
            date=target_date.isoformat(),
            file=str(output_file),
            facts=len(facts),
            transcriptions=len(transcriptions),
            density=metrics.get("information_density_score", 0),
        )
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å —Å –¥–∞–π–¥–∂–µ—Å—Ç–æ–º (–∏–Ω—Å–∞–π—Ç—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
        try:
            core_memory = get_core_memory()
            session_memory = get_session_memory()
            
            # –°–æ–∑–¥–∞—ë–º —Å–µ—Å—Å–∏—é –¥–ª—è –¥–Ω—è
            session_id = f"day_{target_date.isoformat()}"
            session_memory.create_session(session_id, metadata={
                "date": target_date.isoformat(),
                "transcriptions_count": len(transcriptions),
                "facts_count": len(facts),
                "density_score": metrics.get("information_density_score", 0),
            })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –∏–∑ —Ñ–∞–∫—Ç–æ–≤
            for fact in facts[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                session_memory.add_context(session_id, {
                    "type": fact.get("type", "fact"),
                    "text": fact.get("text", ""),
                    "timestamp": fact.get("timestamp"),
                })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º core memory —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
            if facts:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∞—Å—Ç–æ—Ç–∞ —Ç–∏–ø–æ–≤ —Ñ–∞–∫—Ç–æ–≤)
                fact_types = {}
                for fact in facts:
                    fact_type = fact.get("type", "fact")
                    fact_types[fact_type] = fact_types.get(fact_type, 0) + 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–Ω—è
                daily_patterns = core_memory.get("daily_patterns", {})
                daily_patterns[target_date.isoformat()] = {
                    "fact_types": fact_types,
                    "density_score": metrics.get("information_density_score", 0),
                }
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π)
                if len(daily_patterns) > 30:
                    oldest_date = min(daily_patterns.keys())
                    del daily_patterns[oldest_date]
                core_memory.set("daily_patterns", daily_patterns)
            
            logger.info("memory_synced_with_digest", date=target_date.isoformat())
            
        except Exception as e:
            logger.warning("memory_sync_failed", error=str(e), continue_without_sync=True)
        
        return output_file
















