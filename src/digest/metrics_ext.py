"""–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—á–∏."""
from collections import Counter
from typing import List, Dict, Optional
import numpy as np

from src.utils.logging import setup_logging, get_logger

setup_logging()
logger = get_logger("digest.metrics_ext")


def lexical_diversity(texts: List[str]) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (type-token ratio).
    
    –ú–µ—Ä–∞ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–µ–Ω —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å.
    –ó–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1, –≥–¥–µ 1 = –≤—Å–µ —Å–ª–æ–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã.
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        
    Returns:
        –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (0-1)
    """
    if not texts:
        return 0.0
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    all_text = " ".join(texts).lower()
    words = [w.strip(".,!?;:()[]{}'\"") for w in all_text.split() if w.strip()]
    
    if not words:
        return 0.0
    
    unique_words = len(set(words))
    total_words = len(words)
    
    diversity = unique_words / (total_words + 1e-6)
    
    return float(diversity)


def avg_words_per_segment(texts: List[str]) -> float:
    """
    –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç.
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        
    Returns:
        –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    """
    if not texts:
        return 0.0
    
    word_counts = [len(text.split()) for text in texts if text.strip()]
    
    if not word_counts:
        return 0.0
    
    return float(np.mean(word_counts))


def avg_chars_per_segment(texts: List[str]) -> float:
    """
    –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç.
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        
    Returns:
        –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
    """
    if not texts:
        return 0.0
    
    char_counts = [len(text) for text in texts if text.strip()]
    
    if not char_counts:
        return 0.0
    
    return float(np.mean(char_counts))


def hourly_density_variation(density_by_hour: List[float]) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏—é –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –ø–æ —á–∞—Å–∞–º (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ).
    
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.
    –ù–∏–∑–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ = —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –≤—ã—Å–æ–∫–æ–µ = –ø–∏–∫–æ–≤–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.
    
    Args:
        density_by_hour: –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –ø–æ —á–∞—Å–∞–º (0-23)
        
    Returns:
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
    """
    if not density_by_hour:
        return 0.0
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –ª—É—á—à–µ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
    values = np.array(density_by_hour)
    if np.sum(values) == 0:
        return 0.0
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
    std_dev = float(np.std(values))
    
    return std_dev


def wpm_rate(segment_durations: List[float], segment_texts: List[str]) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏ (words per minute).
    
    Args:
        segment_durations: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        segment_texts: –¢–µ–∫—Å—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        
    Returns:
        –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏ (—Å–ª–æ–≤/–º–∏–Ω—É—Ç—É)
    """
    if not segment_durations or not segment_texts:
        return 0.0
    
    rates = []
    for duration, text in zip(segment_durations, segment_texts):
        if duration > 0 and text.strip():
            words = len(text.split())
            wpm = (words / duration) * 60
            rates.append(wpm)
    
    if not rates:
        return 0.0
    
    return float(np.mean(rates))


def semantic_density_score(
    texts: List[str],
    diversity_weight: float = 0.4,
    length_weight: float = 0.3,
    word_density_weight: float = 0.3,
) -> float:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤.
    
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫ –≤ –µ–¥–∏–Ω—É—é –æ—Ü–µ–Ω–∫—É (0-1):
    - –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    - –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤ (—Å–ª–æ–≤–∞ –Ω–∞ —Å–∏–º–≤–æ–ª)
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        diversity_weight: –í–µ—Å –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        length_weight: –í–µ—Å —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã
        word_density_weight: –í–µ—Å –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        
    Returns:
        –û—Ü–µ–Ω–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ (0-1)
    """
    if not texts:
        return 0.0
    
    # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    diversity = lexical_diversity(texts)
    
    # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º: –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–æ—Ä–º–∞ ~50 —Å–ª–æ–≤)
    avg_words = avg_words_per_segment(texts)
    length_score = min(1.0, avg_words / 50.0)
    
    # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤ (—Å–ª–æ–≤ –Ω–∞ —Å–∏–º–≤–æ–ª, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º)
    all_chars = sum(len(t) for t in texts)
    all_words = sum(len(t.split()) for t in texts)
    word_density = (all_words / (all_chars + 1e-6)) * 10  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 10 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    word_density_score = min(1.0, word_density)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º
    score = (
        diversity_weight * diversity +
        length_weight * length_score +
        word_density_weight * word_density_score
    )
    
    return float(score)


def calculate_segmentation_metrics(segments: List[Dict]) -> Dict:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.
    
    Args:
        segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏ 'text', 'duration', 'timestamp'
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    """
    if not segments:
        return {
            "avg_duration": 0.0,
            "avg_words_per_segment": 0.0,
            "total_segments": 0,
            "total_words": 0,
        }
    
    texts = [s.get("text", "") for s in segments]
    durations = [s.get("duration", 0) or 0 for s in segments]
    
    return {
        "avg_duration": float(np.mean(durations)) if durations else 0.0,
        "avg_words_per_segment": avg_words_per_segment(texts),
        "total_segments": len(segments),
        "total_words": sum(len(t.split()) for t in texts),
    }


def calculate_extended_metrics(
    transcriptions: List[Dict],
    hourly_distribution: Optional[Dict[str, int]] = None,
    enabled: bool = True,
) -> Dict:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
    
    Args:
        transcriptions: –°–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —Å –ø–æ–ª—è–º–∏ 'text', 'duration', 'created_at'
        hourly_distribution: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –ø–æ —á–∞—Å–∞–º
        enabled: –í–∫–ª—é—á–µ–Ω—ã –ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    if not enabled:
        return {}
    
    if not transcriptions:
        return {
            "lexical_diversity": 0.0,
            "semantic_density": 0.0,
            "avg_words_per_segment": 0.0,
            "avg_chars_per_segment": 0.0,
            "wpm_rate": 0.0,
            "hourly_variation": 0.0,
            "segmentation": {},
        }
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    texts = [t.get("text", "") for t in transcriptions if t.get("text", "").strip()]
    durations = [t.get("duration", 0) or 0 for t in transcriptions]
    
    if not texts:
        return {
            "lexical_diversity": 0.0,
            "semantic_density": 0.0,
            "avg_words_per_segment": 0.0,
            "avg_chars_per_segment": 0.0,
            "wpm_rate": 0.0,
            "hourly_variation": 0.0,
            "segmentation": {},
        }
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    diversity = lexical_diversity(texts)
    avg_words = avg_words_per_segment(texts)
    avg_chars = avg_chars_per_segment(texts)
    semantic_density = semantic_density_score(texts)
    wpm = wpm_rate(durations, texts)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    segmentation = calculate_segmentation_metrics([
        {
            "text": t.get("text", ""),
            "duration": t.get("duration", 0) or 0,
            "timestamp": t.get("created_at", ""),
        }
        for t in transcriptions
    ])
    
    # –í–∞—Ä–∏–∞—Ü–∏—è –ø–æ —á–∞—Å–∞–º
    hourly_var = 0.0
    if hourly_distribution:
        # –°–æ–∑–¥–∞—ë–º –º–∞—Å—Å–∏–≤ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –ø–æ —á–∞—Å–∞–º (0-23)
        hours = [int(h) for h in hourly_distribution.keys() if h.isdigit()]
        if hours:
            max_hour = max(hours) if hours else 23
            density_array = [hourly_distribution.get(str(h), 0) for h in range(max_hour + 1)]
            hourly_var = hourly_density_variation(density_array)
    
    return {
        "lexical_diversity": round(diversity, 3),
        "semantic_density": round(semantic_density, 3),
        "avg_words_per_segment": round(avg_words, 1),
        "avg_chars_per_segment": round(avg_chars, 1),
        "wpm_rate": round(wpm, 1),
        "hourly_variation": round(hourly_var, 3),
        "segmentation": segmentation,
    }


def interpret_semantic_density(score: float) -> str:
    """
    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç –æ—Ü–µ–Ω–∫—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏.
    
    Args:
        score: –û—Ü–µ–Ω–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ (0-1)
        
    Returns:
        –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    """
    if score >= 0.7:
        return "üî¥ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è ‚Äî –æ—á–µ–Ω—å –Ω–∞—Å—ã—â–µ–Ω–Ω–∞—è —Ä–µ—á—å —Å –±–æ–≥–∞—Ç—ã–º —Å–ª–æ–≤–∞—Ä—ë–º"
    elif score >= 0.5:
        return "üü† –í—ã—Å–æ–∫–∞—è ‚Äî —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è —Ä–µ—á—å"
    elif score >= 0.3:
        return "üü° –°—Ä–µ–¥–Ω—è—è ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ä–µ—á—å"
    elif score >= 0.15:
        return "üü¢ –ù–∏–∑–∫–∞—è ‚Äî –ø—Ä–æ—Å—Ç–∞—è, –ø–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è —Ä–µ—á—å"
    else:
        return "‚ö™ –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ"


def interpret_wpm_rate(wpm: float) -> str:
    """
    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏.
    
    Args:
        wpm: –°–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏ (—Å–ª–æ–≤/–º–∏–Ω—É—Ç—É)
        
    Returns:
        –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    """
    if wpm >= 180:
        return "–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è"
    elif wpm >= 150:
        return "–ë—ã—Å—Ç—Ä–∞—è"
    elif wpm >= 120:
        return "–°—Ä–µ–¥–Ω—è—è"
    elif wpm >= 90:
        return "–ú–µ–¥–ª–µ–Ω–Ω–∞—è"
    else:
        return "–û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–∞—è"













