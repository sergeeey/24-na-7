"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∏–∑ —Ç–µ—Å—Ç–æ–≤ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —á–µ–∫–ª–∏—Å—Ç–∞.
Reflexio v2.1 ‚Äî Surpass Smart Noter Sprint

–°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑:
- pytest JSON –æ—Ç—á—ë—Ç–æ–≤
- –õ–æ–≥–æ–≤ —Ç–µ—Å—Ç–æ–≤
- –ü—Ä—è–º—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π (WER, Latency, etc.)
"""
import json
import yaml
import re
from pathlib import Path
from typing import Dict, Any, Optional, List
import sys

def load_checklist(checklist_path: Path) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–µ–∫–ª–∏—Å—Ç."""
    with open(checklist_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def save_checklist(checklist: Dict[str, Any], checklist_path: Path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ–∫–ª–∏—Å—Ç."""
    with open(checklist_path, "w", encoding="utf-8") as f:
        yaml.dump(checklist, f, allow_unicode=True, default_flow_style=False, sort_keys=False)

def parse_pytest_json_report(report_path: Path) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç pytest JSON –æ—Ç—á—ë—Ç."""
    if not report_path.exists():
        return {}
    
    with open(report_path, "r", encoding="utf-8") as f:
        return json.load(f)

def extract_wer_from_tests(report: Dict[str, Any]) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç WER –∏–∑ —Ç–µ—Å—Ç–æ–≤ ASR accuracy."""
    # –ò—â–µ–º –≤ summary –∏–ª–∏ –≤ –ª–æ–≥–∞—Ö —Ç–µ—Å—Ç–æ–≤
    tests = report.get("tests", [])
    for test in tests:
        if "asr_accuracy" in test.get("nodeid", "").lower():
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ WER –≤ –≤—ã–≤–æ–¥–µ
            call = test.get("call", {})
            stdout = call.get("stdout", "")
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "WER: X.X%"
            match = re.search(r'WER[:\s]+([\d.]+)%?', stdout, re.IGNORECASE)
            if match:
                return f"{match.group(1)}%"
    
    return None

def extract_latency_from_tests(report: Dict[str, Any]) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç Latency –∏–∑ —Ç–µ—Å—Ç–æ–≤ ASR latency."""
    tests = report.get("tests", [])
    for test in tests:
        if "asr_latency" in test.get("nodeid", "").lower():
            call = test.get("call", {})
            stdout = call.get("stdout", "")
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "Latency: X.X —Å–µ–∫" –∏–ª–∏ "X.Xs"
            match = re.search(r'Latency[:\s]+([\d.]+)\s*(—Å–µ–∫|s|sec)', stdout, re.IGNORECASE)
            if match:
                return f"{match.group(1)} —Å–µ–∫"
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ —Å "s"
            match = re.search(r'([\d.]+)\s*(—Å–µ–∫|s|sec)', stdout)
            if match:
                return f"{match.group(1)} —Å–µ–∫"
    
    return None

def extract_coverage_from_report(report: Dict[str, Any]) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç coverage –∏–∑ –æ—Ç—á—ë—Ç–∞."""
    summary = report.get("summary", {})
    total = summary.get("total", 0)
    passed = summary.get("passed", 0)
    
    if total > 0:
        coverage_pct = (passed / total) * 100
        return f"{coverage_pct:.1f}%"
    
    return None

def extract_offline_duration_from_tests(report: Dict[str, Any]) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ñ–ª–∞–π–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏."""
    tests = report.get("tests", [])
    for test in tests:
        if "asr_offline" in test.get("nodeid", "").lower():
            call = test.get("call", {})
            stdout = call.get("stdout", "")
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "‚â• 30 –º–∏–Ω" –∏–ª–∏ "X –º–∏–Ω"
            match = re.search(r'([\d.]+)\s*(–º–∏–Ω|min)', stdout, re.IGNORECASE)
            if match:
                minutes = float(match.group(1))
                return f"‚â• {int(minutes)} –º–∏–Ω"
    
    return None

def extract_deepconf_from_tests(report: Dict[str, Any]) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç DeepConf score –∏–∑ —Ç–µ—Å—Ç–æ–≤."""
    tests = report.get("tests", [])
    for test in tests:
        if "deepconf" in test.get("nodeid", "").lower() or "critic" in test.get("nodeid", "").lower():
            call = test.get("call", {})
            stdout = call.get("stdout", "")
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "DeepConf: 0.XX" –∏–ª–∏ "confidence: 0.XX"
            match = re.search(r'(?:DeepConf|confidence)[:\s]+([\d.]+)', stdout, re.IGNORECASE)
            if match:
                score = float(match.group(1))
                return f"{score:.2f}"
    
    return None

def extract_factual_consistency_from_tests(report: Dict[str, Any]) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç Factual consistency –∏–∑ —Ç–µ—Å—Ç–æ–≤."""
    tests = report.get("tests", [])
    for test in tests:
        if "factual" in test.get("nodeid", "").lower() or "consistency" in test.get("nodeid", "").lower():
            call = test.get("call", {})
            stdout = call.get("stdout", "")
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "Factual consistency: XX%"
            match = re.search(r'Factual[:\s]+([\d.]+)%?', stdout, re.IGNORECASE)
            if match:
                return f"{match.group(1)}%"
    
    return None

def extract_token_entropy_from_tests(report: Dict[str, Any]) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç Token entropy –∏–∑ —Ç–µ—Å—Ç–æ–≤."""
    tests = report.get("tests", [])
    for test in tests:
        if "entropy" in test.get("nodeid", "").lower():
            call = test.get("call", {})
            stdout = call.get("stdout", "")
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "Token entropy: 0.XX"
            match = re.search(r'Token[:\s]+entropy[:\s]+([\d.]+)', stdout, re.IGNORECASE)
            if match:
                return match.group(1)
    
    return None

def update_metric_in_checklist(
    checklist: Dict[str, Any],
    epic_key: str,
    metric_name: str,
    value: Any,
) -> bool:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫—É –≤ —á–µ–∫–ª–∏—Å—Ç–µ."""
    epic = checklist.get("epics", {}).get(epic_key)
    if not epic:
        return False
    
    metrics = epic.get("metrics", [])
    for metric in metrics:
        if metric.get("name") == metric_name:
            metric["current"] = value
            return True
    
    return False

def auto_update_metrics_from_report(
    checklist_path: Path,
    report_path: Path,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ —á–µ–∫–ª–∏—Å—Ç–µ –∏–∑ pytest –æ—Ç—á—ë—Ç–∞.
    
    Returns:
        {
            "updated": List[str],  # –°–ø–∏—Å–æ–∫ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            "skipped": List[str],  # –ú–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏
        }
    """
    checklist = load_checklist(checklist_path)
    report = parse_pytest_json_report(report_path)
    
    updated = []
    skipped = []
    
    # –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    metric_extractors = {
        ("epic_i_asr", "WER"): extract_wer_from_tests,
        ("epic_i_asr", "Latency"): extract_latency_from_tests,
        ("epic_i_asr", "–û—Ñ–ª–∞–π–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è"): extract_offline_duration_from_tests,
        ("epic_ii_llm", "Factual consistency"): extract_factual_consistency_from_tests,
        ("epic_ii_llm", "DeepConf score"): extract_deepconf_from_tests,
        ("epic_ii_llm", "Token entropy"): extract_token_entropy_from_tests,
    }
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    for (epic_key, metric_name), extractor in metric_extractors.items():
        value = extractor(report)
        if value:
            if not dry_run:
                update_metric_in_checklist(checklist, epic_key, metric_name, value)
            updated.append(f"{epic_key}:{metric_name} = {value}")
        else:
            skipped.append(f"{epic_key}:{metric_name}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ª–∏—Å—Ç
    if not dry_run and updated:
        save_checklist(checklist, checklist_path)
    
    return {
        "updated": updated,
        "skipped": skipped,
    }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Auto-update metrics from test reports")
    parser.add_argument(
        "--checklist",
        default=".cursor/tasks/surpass_smart_noter_checklist.yaml",
        help="Path to checklist YAML file",
    )
    parser.add_argument(
        "--report",
        default="tests/.report.json",
        help="Path to pytest JSON report",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Don't update checklist, just show what would be updated",
    )
    
    args = parser.parse_args()
    
    checklist_path = Path(args.checklist)
    report_path = Path(args.report)
    
    if not checklist_path.exists():
        print(f"‚ùå Checklist not found: {checklist_path}")
        sys.exit(1)
    
    if not report_path.exists():
        print(f"‚ö†Ô∏è  Report not found: {report_path}")
        print("   Run tests with: pytest --json-report --json-report-file=tests/.report.json")
        sys.exit(0)
    
    result = auto_update_metrics_from_report(
        checklist_path,
        report_path,
        dry_run=args.dry_run,
    )
    
    if args.dry_run:
        print("üîç Dry-run mode ‚Äî no changes made")
        print()
    
    if result["updated"]:
        print("‚úÖ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
        for item in result["updated"]:
            print(f"  - {item}")
    else:
        print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
    
    if result["skipped"]:
        print(f"\n‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –æ—Ç—á—ë—Ç–µ): {len(result['skipped'])}")
        if len(result["skipped"]) <= 5:
            for item in result["skipped"]:
                print(f"  - {item}")
    
    if not args.dry_run and result["updated"]:
        print(f"\nüíæ –ß–µ–∫–ª–∏—Å—Ç –æ–±–Ω–æ–≤–ª—ë–Ω: {checklist_path}")

if __name__ == "__main__":
    main()





